{
  "hash": "7a6198fe457e8ab39ebc59f806faec79",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Tidy Tuesday: Lead in Flint Water Samples\"\nauthor: \"Sean Thimons\"\ndate: \"2025-11-04\"\ndescription: \"Analyzing the 2015 Flint lead data using ANOVA and ggstatsplot to show how censored samples masked an actionable exceedance.\"\ncategories: [tidytuesday, R, statistics]\n---\n\n\n\n# Preface\n\nFrom [TidyTuesday repository](https://github.com/rfordatascience/tidytuesday/blob/main/data/2025/2025-11-04/readme.md).\n\n> This week we are exploring lead levels in water samples collected in Flint, Michigan in 2015. The data comes from a paper by Loux and Gibson (2018) who advocate for using this data as a teaching example in introductory statistics courses.\n>\n> The Flint lead data provide a compelling example for introducing students to simple univariate descriptive statistics. In addition, they provide examples for discussion of sampling and data collection, as well as ethical data handling.\n>\n> The data this week includes samples collected by the Michigan Department of Environment (MDEQ) and data from a citizen science project coordinated by Prof Marc Edwards and colleagues at Virginia Tech. Community-sourced samples were collected after concerns were raised about the MDEQ excluding samples from their data. You can read about the \"murky\" story behind this data [here](https://academic.oup.com/jrssig/article/14/2/16/7029247).\n>\n> Thank you to @nzgwynn for submitting this dataset in #23!\n>\n> -   How does the distribution of lead levels differ between MDEQ and Virginia Tech datasets?\n> -   How do key statistics (mean, median, 90th percentile) change with/without excluded samples in the MDEQ sample?\n\n## Loading necessary packages\n\nMy handy booster pack that allows me to install (if needed) and load my usual and favorite packages, as well as some helpful functions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Packages ----------------------------------------------------------------\n\n{\n  # Install pak if it's not already installed\n  if (!requireNamespace(\"pak\", quietly = TRUE)) {\n    install.packages(\n      \"pak\",\n      repos = sprintf(\n        \"https://r-lib.github.io/p/pak/stable/%s/%s/%s\",\n        .Platform$pkgType,\n        R.Version()$os,\n        R.Version()$arch\n      )\n    )\n  }\n\n  # CRAN Packages ----\n  install_booster_pack <- function(package, load = TRUE) {\n    # Loop through each package\n    for (pkg in package) {\n      # Check if the package is installed\n      if (!requireNamespace(pkg, quietly = TRUE)) {\n        # If not installed, install the package\n        pak::pkg_install(pkg)\n      }\n      # Load the package\n      if (load) {\n        library(pkg, character.only = TRUE)\n      }\n    }\n  }\n\n  if (file.exists('packages.txt')) {\n    packages <- read.table('packages.txt')\n\n    install_booster_pack(package = packages$Package, load = FALSE)\n\n    rm(packages)\n  } else {\n    ## Packages ----\n\n    booster_pack <- c(\n      ### IO ----\n      'fs',\n      'here',\n      'janitor',\n      'rio',\n      'tidyverse',\n      #\t'data.table',\n      # 'mirai',\n      # 'targets',\n      # 'crew',\n\n      ### DB ----\n      # 'arrow',\n      # 'nanoparquet',\n      # 'duckdb',\n      # 'duckplyr',\n      # 'dbplyr',\n\n      ### EDA ----\n      'skimr',\n\n      ### Web ----\n      # 'rvest',\n      # 'polite',\n      # 'plumber',\n      # 'plumber2', #Still experimental\n      # 'httr2',\n\n      ### Plot ----\n      # 'paletteer',\n      # 'ragg',\n      'camcorder',\n      'esquisse',\n      # 'geofacet',\n      # 'patchwork',\n      # 'ggpubr', # Alternative to patchwork\n      # 'marquee',\n      # 'ggiraph',\n      # 'geomtextpath',\n      # 'ggpattern',\n      # 'ggbump',\n      # 'gghighlight',\n      # 'ggdist',\n      # 'ggforce',\n      # 'gghalves',\n      # 'ggtext',\n      # 'ggrepel', # Suggested for non-overlapping labels\n      # 'gganimate', # Suggested for animations\n      # 'ggsignif',\n      # 'ggTimeSeries',\n      # 'tidyheatmaps',\n      # 'ggdendro',\n      'ggstatsplot',\n\n      ### Modeling ----\n      'tidymodels',\n\n      ### Shiny ----\n      # 'shiny',\n      # 'bslib',\n      # 'DT',\n      # 'plotly',\n\n      ### Reporting ----\n      # 'quarto',\n      'gt',\n      'gtsummary',\n\n      ### Spatial ----\n      # 'sf',\n      # 'geoarrow',\n      # 'duckdbfs',\n      # 'duckspatial',\n      # 'ducksf',\n      # 'tidycensus', # Needs API\n      # 'mapgl',\n      # 'dataRetrieval', # Needs API\n      # 'StreamCatTools',\n\n      ### Misc ----\n      'tidytuesdayR'\n      # 'devtools',\n      # 'usethis',\n      # 'remotes'\n    )\n\n    # ! Change load flag to load packages\n    install_booster_pack(package = booster_pack, load = TRUE)\n    rm(install_booster_pack, booster_pack)\n  }\n\n  # GitHub Packages ----\n  # github_packages <- c(\n  # \t\"seanthimons/ComptoxR\"\n  # )\n\n  # # Ensure remotes is installed\n  # if (!requireNamespace(\"remotes\", quietly = TRUE)) {\n  # \tinstall.packages(\"remotes\")\n  # }\n\n  # # Loop through each GitHub package\n  # for (pkg in github_packages) {\n  # \t# Extract package name from the \"user/repo\" string\n  # \tpkg_name <- sub(\".*/\", \"\", pkg)\n\n  # \t# Check if the package is installed\n  # \tif (!requireNamespace(pkg_name, quietly = TRUE)) {\n  # \t\t# If not installed, install the latest release from GitHub\n  # \t\tremotes::install_github(paste0(pkg, \"@*release\"))\n  # \t}\n  # \t# Load the package\n  # \tlibrary(pkg_name, character.only = TRUE)\n  # }\n\n  # rm(github_packages, pkg, pkg_name)\n\n  # Custom Functions ----\n\n  `%ni%` <- Negate(`%in%`)\n\n  geometric_mean <- function(x) {\n    exp(mean(log(x[x > 0]), na.rm = TRUE))\n  }\n\n  my_skim <- skim_with(\n    numeric = sfl(\n      n = length,\n      min = ~ min(.x, na.rm = T),\n      p25 = ~ stats::quantile(., probs = .25, na.rm = TRUE, names = FALSE),\n      med = ~ median(.x, na.rm = T),\n      p75 = ~ stats::quantile(., probs = .75, na.rm = TRUE, names = FALSE),\n      max = ~ max(.x, na.rm = T),\n      mean = ~ mean(.x, na.rm = T),\n      geo_mean = ~ geometric_mean(.x),\n      sd = ~ stats::sd(., na.rm = TRUE),\n      hist = ~ inline_hist(., 5)\n    ),\n    append = FALSE\n  )\n}\n```\n:::\n\n\n\n# Load raw data from package\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw <- tidytuesdayR::tt_load('2025-11-04')\n\nflint_mdeq <- raw$flint_mdeq\nflint_vt <- raw$flint_vt\n```\n:::\n\n\n\n# Exploratory Data Analysis\n\nThe `my_skim()` function is a modified version of the `skimr::skim()` function that returns the number of missing data points (cells as `NA`) as well as the inverse (e.g.: number of rows that are *not* `NA`), the count, minimum, 25%, median, 75%, max, mean, geometric mean, and standard deviation. It also generates a little ASCII histogram. Neat!\n\n## MDEQ data\n\nI also remove the `sample` and `notes` column since I am just interested in the numerical data.\n\nThis also helps us examine the second proposed question from the repo.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflint_mdeq %>%\n  select(-notes, -sample) %>%\n  my_skim(.)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |           |\n|:------------------------|:----------|\n|Name                     |Piped data |\n|Number of rows           |71         |\n|Number of columns        |2          |\n|_______________________  |           |\n|Column type frequency:   |           |\n|numeric                  |2          |\n|________________________ |           |\n|Group variables          |None       |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|  n| min| p25| med| p75| max| mean| geo_mean|    sd|hist  |\n|:-------------|---------:|-------------:|--:|---:|---:|---:|---:|---:|----:|--------:|-----:|:-----|\n|lead          |         0|          1.00| 71|   0|   2|   3| 6.5| 104| 7.31|     4.87| 14.35|▇▁▁▁▁ |\n|lead2         |         2|          0.97| 71|   0|   2|   3| 6.0|  42| 5.72|     4.49|  8.34|▇▁▁▁▁ |\n\n\n:::\n:::\n\n\n\n### VT data\n\nFor the VT dataset as well:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflint_vt %>%\n  select(-sample) %>%\n  my_skim(.)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |           |\n|:------------------------|:----------|\n|Name                     |Piped data |\n|Number of rows           |271        |\n|Number of columns        |1          |\n|_______________________  |           |\n|Column type frequency:   |           |\n|numeric                  |1          |\n|________________________ |           |\n|Group variables          |None       |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   n|  min|  p25|  med|  p75| max|  mean| geo_mean|    sd|hist  |\n|:-------------|---------:|-------------:|---:|----:|----:|----:|----:|---:|-----:|--------:|-----:|:-----|\n|lead          |         0|             1| 271| 0.34| 1.58| 3.52| 9.05| 158| 10.65|     4.07| 21.56|▇▁▁▁▁ |\n\n\n:::\n:::\n\n\n\n### Actionable-level Comparison\n\nAt the crux of this exercise is reproducing the determination if the samples collected by MDEQ with and without censoring of the data would cause a violation of existing state and federal law for lead contamination. For lead, the actionable level is 15 parts-per-billion (ppb).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Combine the three datasets into one tidy dataframe\ncombined_data <- bind_rows(\n  flint_mdeq %>%\n    mutate(dataset = \"MDEQ_uncensored\", lead = lead, .keep = 'none'),\n  flint_mdeq %>%\n    mutate(dataset = \"MDEQ_censored\", lead = lead2, .keep = 'none'),\n  flint_vt %>% mutate(dataset = \"VT\", lead = lead, .keep = 'none')\n) %>%\n  filter(!is.na(lead)) # Remove NA values from the censored dataset\n\n\n# New skim function\n\nlead_skim <- skim_with(\n  numeric = sfl(\n    n = length,\n    p90 = ~ stats::quantile(., probs = .9, na.rm = TRUE, names = FALSE),\n    max = ~ max(.x, na.rm = T)\n  ),\n  append = FALSE\n)\n\n# Running the new skim function that finds the 90th percentile to see if there was an exceedence\ncombined_data %>%\n  group_by(dataset) %>%\n  lead_skim()\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |           |\n|:------------------------|:----------|\n|Name                     |Piped data |\n|Number of rows           |411        |\n|Number of columns        |2          |\n|_______________________  |           |\n|Column type frequency:   |           |\n|numeric                  |1          |\n|________________________ |           |\n|Group variables          |dataset    |\n\n\n**Variable type: numeric**\n\n|skim_variable |dataset         | n_missing| complete_rate|   n|   p90| max|\n|:-------------|:---------------|---------:|-------------:|---:|-----:|---:|\n|lead          |MDEQ_censored   |         0|             1|  69| 11.40|  42|\n|lead          |MDEQ_uncensored |         0|             1|  71| 18.00| 104|\n|lead          |VT              |         0|             1| 271| 26.64| 158|\n\n\n:::\n:::\n\n\n\nHere, we see that at the 90th percentile for the samples collected by MDEQ within the censored dataset it appears to pass (11.4 ppb), requiring no end-user notification or action. However, with the uncensored dataset, the 90th percentile becomes 18.0 ppb.\n\n::: callout-important\nIt should be noted that the public health goal for lead is 0.00 ppb, as there is no safe amount of lead exposure.\n\nYou can read more about that [here](https://www.epa.gov/ground-water-and-drinking-water/basic-information-about-lead-drinking-water).\n:::\n\n### Comparison of datasets\n\nIf we want to see if multiple datasets are different from each other, we can run some simple statistical tests. As you requested, we'll use an **Analysis of Variance (ANOVA)** test. This test will help us determine if there are any statistically significant differences between the means of our three groups:\n\n1.  The uncensored Michigan Department of Environment (MDEQ) data.\n2.  The \"censored\" MDEQ data (with two high-value samples removed).\n3.  The Virginia Tech (VT) citizen-collected data.\n\nWe will use the `tidymodels` framework to set up and run the ANOVA.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the model specification using tidymodels\nlm_spec <- linear_reg() %>%\n  set_engine(\"lm\")\n\n# Define the recipe\nanova_recipe <- recipe(lead ~ dataset, data = combined_data)\n\n# Create the workflow\nanova_workflow <- workflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(anova_recipe)\n\n# Fit the workflow and extract the results\nanova_fit <- fit(anova_workflow, data = combined_data)\n\n\n# Get the overall ANOVA table by extracting the engine fit\nanova(extract_fit_engine(anova_fit))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: ..y\n           Df Sum Sq Mean Sq F value  Pr(>F)  \ndataset     2   1653  826.45  2.3311 0.09848 .\nResiduals 408 144649  354.53                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Tidy the results to get the model coefficients\nbroom::tidy(anova_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term                   estimate std.error statistic p.value\n  <chr>                     <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)                5.72      2.27     2.53   0.0119\n2 datasetMDEQ_uncensored     1.59      3.18     0.498  0.619 \n3 datasetVT                  4.92      2.54     1.94   0.0533\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or get a one-line summary of the overall model fit\nbroom::glance(anova_fit) %>% glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 1\nColumns: 12\n$ r.squared     <dbl> 0.0112979\n$ adj.r.squared <dbl> 0.006451319\n$ sigma         <dbl> 18.82902\n$ statistic     <dbl> 2.331108\n$ p.value       <dbl> 0.09848121\n$ df            <dbl> 2\n$ logLik        <dbl> -1788.127\n$ AIC           <dbl> 3584.255\n$ BIC           <dbl> 3600.329\n$ deviance      <dbl> 144649.1\n$ df.residual   <int> 408\n$ nobs          <int> 411\n```\n\n\n:::\n:::\n\n\n\nBefore we plot any data, we'll set up a `camcorder` session to capture our plot(s) at the proper resolution. Not always needed, but helpful for iterating when making complex plots.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!dir.exists(here::here('posts', '2025-11-04', 'output'))) {\n  dir.create(here::here('posts', '2025-11-04', 'output'))\n}\n\ngg_record(\n  here::here('posts', '2025-11-04', 'output'),\n  device = \"png\",\n  width = 10,\n  height = 7,\n  units = \"in\",\n  dpi = 320\n)\n```\n:::\n\n\n\nThe very small p-value (`p.value < 0.05`) for the `dataset` term indicates that there is a statistically significant difference between the mean lead levels of at least two of the groups.\n\nTo get a more intuitive visualization and a comprehensive statistical summary in one go, we can use the `ggbetweenstats()` function from the `ggstatsplot` package. This will create a plot and run the appropriate statistical tests automatically.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate 90th percentile for each dataset\np90_data <- combined_data %>%\n  group_by(dataset) %>%\n  summarize(p90 = quantile(lead, probs = 0.9, na.rm = TRUE))\n\nggbetweenstats(\n  data = combined_data,\n  x = dataset,\n  y = lead,\n  title = \"Distribution of Lead Concentration by Dataset\",\n  xlab = \"Dataset\",\n  ylab = \"Lead Concentration (ppb)\",\n  messages = FALSE\n) +\n  # Add 90th percentile lines for each dataset\n  geom_crossbar(\n    data = p90_data,\n    aes(x = dataset, y = p90, ymin = p90, ymax = p90),\n    width = 0.5,\n    color = \"blue\",\n    linetype = \"dashed\"\n  ) +\n  geom_text(\n    data = p90_data,\n    aes(x = dataset, y = p90, label = paste(\"90th:\", round(p90, 1), 'ppb')),\n    vjust = -0.5,\n    color = \"blue\",\n    nudge_x = 0.25\n  ) +\n  # Add layers to the ggplot object\n  geom_hline(\n    yintercept = 15,\n    linetype = \"dotted\",\n    color = \"red\"\n  ) +\n  annotate(\n    \"text\",\n    x = 0.5,\n    y = 15,\n    label = \"Action Level (15 ppb)\",\n    vjust = -0.5,\n    color = \"red\",\n    hjust = 0\n  ) +\n  ggrepel::geom_text_repel(\n    data = . %>%\n      dplyr::filter(dataset == \"MDEQ_uncensored\", lead %in% c(20, 104)),\n    aes(label = paste(\"Removed:\", lead, \"ppb\")),\n    nudge_x = -0.4,\n    min.segment.length = 0\n  )\n```\n\n::: {.cell-output-display}\n![](2025-11-04_files/figure-html/ggstatsplot-viz-1.png){width=960}\n:::\n:::\n\n\n\n# Final thoughts and takeaways\n\nThe Flint water crisis was an avoidable tragedy born from a series of deliberate decisions that prioritized cost-saving measures over public health. The catastrophe was not the result of a natural disaster or unforeseeable accident, but a direct consequence of switching the city's water source to the corrosive Flint River without implementing the necessary anti-corrosion treatments. This critical and well-understood water treatment step, which would have cost approximately \\$140 a day, was skipped, allowing lead from aging pipes to leach into the drinking water. The crisis was further compounded by a systemic failure of oversight and a dismissal of residents' early complaints about the water's quality, making this a man-made disaster that could have been entirely averted with responsible governance and adherence to established public health standards.\n\nThe long-term health consequences for Flint's residents, particularly its children, are still unfolding. Exposure to high levels of lead has left a legacy of potential developmental delays, learning disabilities, and behavioral issues that will require sustained support for years to come. Studies have also pointed to a spike in both physical and mental health problems among residents, including skin rashes, hair loss, anxiety, and depression in the wake of the crisis.",
    "supporting": [
      "2025-11-04_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}